#File utilizing LM Studio as a server to utilize LLM to output result to terminal

import json
import random
from openai import OpenAI

# Connection with local server
client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")
model = "llama-3.2-3b-instruct"


def generate_receipt(order_id: str):
    """
    Generates a random receipt for a given order:
    - Random number of items between 1 and 10
    - Each item has a random price between $1.00 and $100.00
    Returns a dict with order_id, num_items, items, and total_price.
    - In practical applications, this would be 
    """
    num_items = random.randint(1, 10)
    items = []
    total = 0.0

    for i in range(num_items):
        price = round(random.uniform(1.0, 100.0), 2)
        items.append({
            "name": f"item_{i+1}",
            "price": price
        })
        total += price

    total = round(total, 2)
    receipt = {
        "order_id": order_id,
        "num_items": num_items,
        "items": items,
        "total_price": total
    }
    #This prints out the random receipt that was generated by the function
    print(f"\ngenerate_receipt returns:\n\n{json.dumps(receipt, indent=2)}", flush=True)
    return receipt

#These are the tools that I am making available to the LLM
tools = [
    {
        "type": "function",
        "function": {
            "name": "generate_receipt",
            "description": "Generate a random receipt for the given order_id. Returns item count, line-items, and total price.",
            "parameters": {
                "type": "object",
                "properties": {
                    "order_id": {
                        "type": "string",
                        "description": "The customer's order ID to generate a receipt for."
                    }
                },
                "required": ["order_id"],
                "additionalProperties": False
            }
        }
    }
]

#These are the prompts that are being inputted to the LLM, this can change depending on the situation and the application of the LLM
messages = [
    {
        "role": "system",
        "content": "You are the backend to a server that stores data. Return the number of items and the total price."
    },
    {
        "role": "user",
        "content": "Please give me a receipt for order number 1017."
    },
]

# 1) Ask the LLM which tool to call:
response = client.chat.completions.create(
    model=model,
    messages=messages,
    tools=tools,
)

# 2) Extract the tool call request from the LLM:
tool_call = response.choices[0].message.tool_calls[0]
arguments = json.loads(tool_call.function.arguments)
order_id = arguments["order_id"]

# 3) Call your local function to generate the receipt:
receipt = generate_receipt(order_id)

# 4) Package the function result back into a message:
assistant_tool_call = {
    "role": "assistant",
    "tool_calls": [
        {
            "id": tool_call.id,
            "type": tool_call.type,
            "function": tool_call.function
        }
    ]
}
function_result_message = {
    "role": "tool",
    "content": json.dumps(receipt),
    "tool_call_id": tool_call.id
}

# 5) Send the conversation + function result back to the LLM so it can respond:
final_response = client.chat.completions.create(
    model=model,
    messages=[
        messages[0],
        messages[1],
        assistant_tool_call,
        function_result_message
    ],
)

print("\nFinal model response:\n")
print(final_response.choices[0].message.content, flush=True)
